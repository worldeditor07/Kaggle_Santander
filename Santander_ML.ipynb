{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.metrics import log_loss, roc_auc_score\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.decomposition import PCA, TruncatedSVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started Loading data...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "print('Started Loading data...')\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rename the columns and extract the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target = train['TARGET']\n",
    "train.drop(['TARGET'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Duplicate Features, including zero columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing Duplicate Columns....\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "print 'Removing Duplicate Columns....'\n",
    "remove = []\n",
    "c = train.columns\n",
    "for i in range(len(c)-1):\n",
    "    v = train[c[i]].values\n",
    "    for j in range(i+1, len(c)):\n",
    "        if np.array_equal(v, train[c[j]].values):\n",
    "            remove.append(c[j])\n",
    "train.drop(remove, axis=1, inplace=True)\n",
    "test.drop(remove, axis=1, inplace=True)\n",
    "print 'Done!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### remove constant features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removing constant columns....\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "print 'Removing constant columns....'\n",
    "remove = []\n",
    "for col in train.columns:\n",
    "    if len(train[col].unique()) == 0:\n",
    "        remove.append(col)\n",
    "train.drop(remove, axis=1, inplace=True)\n",
    "test.drop(remove, axis=1, inplace=True)\n",
    "print 'Done!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### add a feature that counts number of assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "original_features = train.columns[1:-1]\n",
    "train['SumZeros'] = (train[original_features] == 0).sum(axis=1)\n",
    "test['SumZeros'] = (test[original_features] == 0).sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generate PCA features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating PCA features\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "df = pd.concat([train, test], axis = 0)\n",
    "df.shape\n",
    "n_train = train.shape[0] # for split the df back into train, test later\n",
    "n_test = test.shape[0]\n",
    "\n",
    "def generate_PCA_feature(train, test, original_feature, n_components = 4):\n",
    "    \"\"\"\n",
    "    we fit a PCA decomposition model to training set of the shape\n",
    "    : n_train_samples * n_features\n",
    "    and transform on both training set and testing test. \n",
    "    \"\"\"\n",
    "    pca = PCA(n_components = n_components)\n",
    "    # we need to normalize the data before fitting\n",
    "    train_projected = pca.fit_transform(normalize(train[original_features], axis=0))\n",
    "    test_projected = pca.transform(normalize(test[original_features], axis=0))\n",
    "    for i in xrange(1, n_components + 1):\n",
    "        name = 'PCA{:02d}'.format(i)\n",
    "        train[name] = train_projected[:, i - 1]\n",
    "        test[name] = test_projected[:, i - 1]\n",
    "    return train, test\n",
    "print 'Generating PCA features'\n",
    "train, test = generate_PCA_feature(train, test, original_features, n_components = 2)\n",
    "print 'Done!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(76020, 311)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape # two more features added, as expected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### truncated SVD features for nonlinear clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating SVD features\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# we perform cross validation to set the n_components\n",
    "def generate_SVD_feature(train, test, original_features, n_components = 5):\n",
    "    \"\"\"\n",
    "    we fit a PCA decomposition model to training set of the shape\n",
    "    : n_train_samples * n_features\n",
    "    and transform on both training set and testing test. \n",
    "    \"\"\"\n",
    "    svd = TruncatedSVD(n_components = n_components)\n",
    "    # we need to normalize the data before fitting\n",
    "    train_projected = svd.fit_transform(train[original_features])\n",
    "    test_projected = svd.transform(test[original_features])\n",
    "    for i in xrange(1, n_components + 1):\n",
    "        name = 'SVD{:02d}'.format(i)\n",
    "        train[name] = train_projected[:, i - 1]\n",
    "        test[name] = test_projected[:, i - 1]\n",
    "    return train, test\n",
    "print 'Generating SVD features'\n",
    "train, test = generate_SVD_feature(train, test, original_features, n_components = 5)\n",
    "print 'Done!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(76020, 316)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape # 5 more features added, as expected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10-fold cross validation splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features = train.columns[1:-1]\n",
    "split = 10\n",
    "skf = StratifiedKFold(target,\n",
    "                      n_folds=split,\n",
    "                      shuffle=False,\n",
    "                      random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set parameters for XGBosst Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_rounds = 350\n",
    "params = {}\n",
    "params[\"objective\"] = \"binary:logistic\"\n",
    "params[\"eta\"] = 0.03\n",
    "params[\"subsample\"] = 0.8\n",
    "params[\"colsample_bytree\"] = 0.7\n",
    "params[\"silent\"] = 1\n",
    "params[\"max_depth\"] = 5\n",
    "params[\"min_child_weight\"] = 1\n",
    "params[\"eval_metric\"] = \"auc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'ID', u'var3', u'var15', u'imp_ent_var16_ult1',\n",
       "       u'imp_op_var39_comer_ult1', u'imp_op_var39_comer_ult3',\n",
       "       u'imp_op_var40_comer_ult1', u'imp_op_var40_comer_ult3',\n",
       "       u'imp_op_var40_efect_ult1', u'imp_op_var40_efect_ult3',\n",
       "       ...\n",
       "       u'saldo_medio_var44_ult3', u'var38', u'SumZeros', u'PCA01', u'PCA02',\n",
       "       u'SVD01', u'SVD02', u'SVD03', u'SVD04', u'SVD05'],\n",
       "      dtype='object', length=316)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train['target'] = target # add back the label\n",
    "features = train.columns[1:-1] # exclude ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Fold:', 1)\n",
      "('Blind Log Loss:', 0.13505756484651632)\n",
      "('Blind ROC:', 0.83806079615924634)\n",
      "finished a training model\n",
      "fitting on full data set now...\n",
      "('Fold:', 2)\n",
      "('Blind Log Loss:', 0.1341003199256714)\n",
      "('Blind ROC:', 0.83880423239980662)\n",
      "finished a training model\n",
      "fitting on full data set now...\n",
      "('Fold:', 3)\n",
      "('Blind Log Loss:', 0.14165757352950137)\n",
      "('Blind ROC:', 0.81413527751397996)\n",
      "finished a training model\n",
      "fitting on full data set now...\n",
      "('Fold:', 4)\n",
      "('Blind Log Loss:', 0.13285799161095355)\n",
      "('Blind ROC:', 0.84142344310909944)\n",
      "finished a training model\n",
      "fitting on full data set now...\n",
      "('Fold:', 5)\n",
      "('Blind Log Loss:', 0.13225275204203996)\n",
      "('Blind ROC:', 0.84591311161580274)\n",
      "finished a training model\n",
      "fitting on full data set now...\n",
      "('Fold:', 6)\n",
      "('Blind Log Loss:', 0.13416997346358403)\n",
      "('Blind ROC:', 0.84142412567158464)\n",
      "finished a training model\n",
      "fitting on full data set now...\n",
      "('Fold:', 7)\n",
      "('Blind Log Loss:', 0.13143148793218037)\n",
      "('Blind ROC:', 0.85123163850034667)\n",
      "finished a training model\n",
      "fitting on full data set now...\n",
      "('Fold:', 8)\n",
      "('Blind Log Loss:', 0.12792796936474868)\n",
      "('Blind ROC:', 0.8600903894747044)\n",
      "finished a training model\n",
      "fitting on full data set now...\n",
      "('Fold:', 9)\n",
      "('Blind Log Loss:', 0.13024150338882812)\n",
      "('Blind ROC:', 0.85134730402228009)\n",
      "finished a training model\n",
      "fitting on full data set now...\n",
      "('Fold:', 10)\n",
      "('Blind Log Loss:', 0.13707694026894318)\n",
      "('Blind ROC:', 0.82787586175409766)\n",
      "finished a training model\n",
      "fitting on full data set now...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "train_preds = None\n",
    "test_preds = None\n",
    "xgb_classifiers = [] # List[[clf, score]]\n",
    "for index, (train_index, test_index) in enumerate(skf):\n",
    "    print('Fold:', index)\n",
    "    X_train = train.iloc[train_index] # split training set into'train', 'cross-validation' sets\n",
    "    X_test = train.iloc[test_index]\n",
    "    \n",
    "    # for xgb classifier, we transform them into DMatrix format\n",
    "    D_train = xgb.DMatrix(\n",
    "                    csr_matrix(X_train[features]),\n",
    "                    X_train.target.values,\n",
    "                    silent=True)\n",
    "    \n",
    "    D_test = xgb.DMatrix(\n",
    "                    csr_matrix(X_test[features]),\n",
    "                    X_test.target.values,\n",
    "                    silent=True)\n",
    "    watchlist = [(D_test, 'eval'), (D_train, 'train')]\n",
    "    \n",
    "    # fit the classfier now\n",
    "    clf = xgb.train(params, D_train, num_rounds,\n",
    "                    evals = watchlist, early_stopping_rounds=50,\n",
    "                    verbose_eval=False)\n",
    "\n",
    "    test_prediction = clf.predict(D_test)\n",
    "    print('Blind Log Loss:', log_loss(X_test.target.values,\n",
    "                                      test_prediction))\n",
    "    score = roc_auc_score(X_test.target.values,\n",
    "                                      test_prediction)\n",
    "    print('Blind ROC:', score)\n",
    "    index = index + 1\n",
    "    \n",
    "    del X_train, X_test, D_train, D_test\n",
    "    gc.collect()\n",
    "    print 'finished a training model'\n",
    "    print 'fitting on full data set now...'\n",
    "    \n",
    "    D_full_train = \\\n",
    "        xgb.DMatrix(csr_matrix(train[features]),\n",
    "                    train.target.values,\n",
    "                    silent=True)\n",
    "    D_full_test = \\\n",
    "        xgb.DMatrix(csr_matrix(test[features]),\n",
    "                    silent=True)\n",
    "    if(train_preds is None):\n",
    "        train_preds = clf.predict(D_full_train)\n",
    "        test_preds = clf.predict(D_full_test)\n",
    "    else:\n",
    "        train_preds *= clf.predict(D_full_train) # we manually perform an average of the results\n",
    "        test_preds *= clf.predict(D_full_test)\n",
    "    xgb_classifiers.append([clf, 'with auc score: {:10f}'.format(score)])\n",
    "    del D_full_train, D_full_test, clf\n",
    "    gc.collect()\n",
    "print 'Done!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save the xgb_classifiers lists for later ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pickle.dump(xgb_classifiers, open('xgboost_classifier_param1.dat', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = pickle.load(open('xgboost_classifier_param1.dat', 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### set another set of parameters, use a randomized search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "split = 5\n",
    "random_state = 111\n",
    "skf = StratifiedKFold(target,\n",
    "                      n_folds=split,\n",
    "                      shuffle=False,\n",
    "                      random_state= random_state)\n",
    "num_rounds = 350 + np.random.randint(low = -50, high = 50)\n",
    "params = {}\n",
    "params[\"objective\"] = \"binary:logistic\"\n",
    "params[\"eta\"] = 0.03 + np.random.normal(loc = 0.0, scale = 0.01)\n",
    "params[\"subsample\"] = 0.8\n",
    "params[\"colsample_bytree\"] = 0.7\n",
    "params[\"silent\"] = 1\n",
    "params[\"max_depth\"] = 5 + np.random.randint(low = -1, high = 2)\n",
    "params[\"min_child_weight\"] = 1\n",
    "params[\"eval_metric\"] = \"auc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Fold:', 0)\n",
      "('Blind Log Loss:', 0.13515509804987907)\n",
      "('Blind ROC:', 0.83580701685336112)\n",
      "finished a training model\n",
      "fitting on full data set now...\n",
      "('Fold:', 1)\n",
      "('Blind Log Loss:', 0.13680937251164763)\n",
      "('Blind ROC:', 0.82864225095512389)\n",
      "finished a training model\n",
      "fitting on full data set now...\n",
      "('Fold:', 2)\n",
      "('Blind Log Loss:', 0.13302371896058762)\n",
      "('Blind ROC:', 0.84442239514816397)\n",
      "finished a training model\n",
      "fitting on full data set now...\n",
      "('Fold:', 3)\n",
      "('Blind Log Loss:', 0.12968091185474287)\n",
      "('Blind ROC:', 0.85590906677247269)\n",
      "finished a training model\n",
      "fitting on full data set now...\n",
      "('Fold:', 4)\n",
      "('Blind Log Loss:', 0.13412376736040632)\n",
      "('Blind ROC:', 0.83843641868857111)\n",
      "finished a training model\n",
      "fitting on full data set now...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "train_preds = None\n",
    "test_preds = None\n",
    "xgb_classifiers2 = []\n",
    "for index, (train_index, test_index) in enumerate(skf):\n",
    "    print('Fold:', index)\n",
    "    X_train = train.iloc[train_index] # split training set into'train', 'cross-validation' sets\n",
    "    X_test = train.iloc[test_index]\n",
    "    \n",
    "    # for xgb classifier, we transform them into DMatrix format\n",
    "    D_train = xgb.DMatrix(\n",
    "                    csr_matrix(X_train[features]),\n",
    "                    X_train.target.values,\n",
    "                    silent=True)\n",
    "    \n",
    "    D_test = xgb.DMatrix(\n",
    "                    csr_matrix(X_test[features]),\n",
    "                    X_test.target.values,\n",
    "                    silent=True)\n",
    "    watchlist = [(D_test, 'eval'), (D_train, 'train')]\n",
    "    \n",
    "    # fit the classfier now\n",
    "    clf = xgb.train(params, D_train, num_rounds,\n",
    "                    evals = watchlist, early_stopping_rounds=50,\n",
    "                    verbose_eval=False)\n",
    "\n",
    "    test_prediction = clf.predict(D_test)\n",
    "    print('Blind Log Loss:', log_loss(X_test.target.values,\n",
    "                                      test_prediction))\n",
    "    score = roc_auc_score(X_test.target.values,\n",
    "                                      test_prediction)\n",
    "    print('Blind ROC:', score)\n",
    "    index = index + 1\n",
    "    \n",
    "    del X_train, X_test, D_train, D_test\n",
    "    gc.collect()\n",
    "    print 'finished a training model'\n",
    "    print 'fitting on full data set now...'\n",
    "    \n",
    "    D_full_train = \\\n",
    "        xgb.DMatrix(csr_matrix(train[features]),\n",
    "                    train.target.values,\n",
    "                    silent=True)\n",
    "    D_full_test = \\\n",
    "        xgb.DMatrix(csr_matrix(test[features]),\n",
    "                    silent=True)\n",
    "    if(train_preds is None):\n",
    "        train_preds = clf.predict(D_full_train)\n",
    "        test_preds = clf.predict(D_full_test)\n",
    "    else:\n",
    "        train_preds *= clf.predict(D_full_train) # we manually perform an average of the results\n",
    "        test_preds *= clf.predict(D_full_test)\n",
    "    xgb_classifiers2.append([clf, 'with auc score: {:10f}'.format(score)])\n",
    "    del D_full_train, D_full_test, clf\n",
    "    gc.collect()\n",
    "print 'Done!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'colsample_bytree': 0.7, 'silent': 1, 'eval_metric': 'auc', 'min_child_weight': 1, 'subsample': 0.8, 'eta': 0.019899065033745854, 'objective': 'binary:logistic', 'max_depth': 5}\n"
     ]
    }
   ],
   "source": [
    "print params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Average Log Loss:', 0.12388091950835343)\n",
      "('Average ROC:', 0.87960446479360432)\n"
     ]
    }
   ],
   "source": [
    "train_preds = np.power(train_preds, 1./split)\n",
    "test_preds = np.power(test_preds, 1./split)\n",
    "print('Average Log Loss:', log_loss(train.target.values, train_preds))\n",
    "print('Average ROC:', roc_auc_score(train.target.values, train_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission = pd.DataFrame({\"ID\": train.ID,\n",
    "                           \"TARGET\": train.target,\n",
    "                           \"PREDICTION\": train_preds})\n",
    "\n",
    "submission.to_csv(\"simplexgbtrain.csv\", index=False)\n",
    "submission = pd.DataFrame({\"ID\": test.ID, \"TARGET\": test_preds})\n",
    "submission.to_csv(\"simplexgbtest.csv\", index=False)\n",
    "print('Finish')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
